{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "\n",
    "Trying out 2 ways of dealing with data:\n",
    "- Option 1 : save croped images and skeleton into pickle files\n",
    "- Option 2 : save croped images and skeleton into a unique h5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset path \n",
    "# ntu_path = \"/home/gnocchi/Documents/datasets/NTU-RGB-D_toy/\"\n",
    "ntu_path = \"/media/gnocchi/Toshiba Ext/NTU-RGB-D/\"\n",
    "\n",
    "rgb_folder = \"nturgb+d_rgb/\"\n",
    "skeleton_folder = \"nturgb+d_skeletons/\"\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "import skvideo.io\n",
    "import skvideo.datasets\n",
    "\n",
    "import pickle\n",
    "import h5py\n",
    "\n",
    "# Custom imports\n",
    "from utils import * \n",
    "from joints import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickle approach "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "start = time.time()\n",
    "\n",
    "# Loop through skeleton files\n",
    "for filename in os.listdir(ntu_path + skeleton_folder):\n",
    "    # Retrieve skeleton data\n",
    "    skeleton = read_xyz(ntu_path + skeleton_folder + filename) # shape (2, max_frame, num_joint=25, n_subjects)\n",
    "    skeleton_rgb = read_color_xy(ntu_path + skeleton_folder + filename) # shape (2, max_frame, num_joint=25, n_subjects)\n",
    "    \n",
    "    # Assert skeletons are correct\n",
    "    assert skeleton.shape[1:] == skeleton_rgb.shape[1:]\n",
    "    \n",
    "    # Sequence code without extension\n",
    "    short_filename = os.path.splitext(filename)[0]\n",
    "    print(short_filename)\n",
    "        \n",
    "    # Read cooresponding video\n",
    "    videodata = skvideo.io.vread(ntu_path + rgb_folder + short_filename +'_rgb.avi') # shape (n_frames, 1080, 1920, 3)\n",
    "    \n",
    "    # Check that video data has same number of frames as skeleton\n",
    "    assert skeleton.shape[1] == videodata.shape[0]\n",
    "    \n",
    "    n_frames = videodata.shape[0]\n",
    "    \n",
    "    # Create empty np arrays containing crops of hands from videos\n",
    "    # shape (n_frames, number of hands (2 subjects), x, y)\n",
    "    hand_crops = np.zeros((n_frames, 4, crop_size, crop_size, 3), dtype=np.uint8)\n",
    "    \n",
    "    left_hand_s0_x = int(skeleton_rgb[1, 0, Joints.HANDLEFT, 0])\n",
    "    left_hand_s0_y = int(skeleton_rgb[0, 0, Joints.HANDLEFT, 0])\n",
    "    \n",
    "    right_hand_s0_x = int(skeleton_rgb[1, 0, Joints.HANDRIGHT, 0])\n",
    "    right_hand_s0_y = int(skeleton_rgb[0, 0, Joints.HANDRIGHT, 0])\n",
    "    \n",
    "    left_hand_s1_x = int(skeleton_rgb[1, 0, Joints.HANDLEFT, 1])\n",
    "    left_hand_s1_y = int(skeleton_rgb[0, 0, Joints.HANDLEFT, 1])\n",
    "    \n",
    "    right_hand_s1_x = int(skeleton_rgb[1, 0, Joints.HANDRIGHT, 1])\n",
    "    right_hand_s1_y = int(skeleton_rgb[0, 0, Joints.HANDRIGHT, 1])\n",
    "    \n",
    "    hand_crops[:, 0] = videodata[:, \n",
    "                                 left_hand_s0_x-offset:left_hand_s0_x+offset, \n",
    "                                 left_hand_s0_y-offset:left_hand_s0_y+offset, \n",
    "                                 :]\n",
    "    hand_crops[:, 1] = videodata[:, \n",
    "                                 right_hand_s0_x-offset:right_hand_s0_x+offset, \n",
    "                                 right_hand_s0_y-offset:right_hand_s0_y+offset, \n",
    "                                 :]\n",
    "    if right_hand_s1_x + right_hand_s1_y + left_hand_s1_x + left_hand_s1_y != 0:\n",
    "        hand_crops[:, 2] = videodata[:, \n",
    "                                     left_hand_s1_x-offset:left_hand_s1_x+offset, \n",
    "                                     left_hand_s1_y-offset:left_hand_s1_y+offset, \n",
    "                                     :]\n",
    "        hand_crops[:, 3] = videodata[:, \n",
    "                                     right_hand_s1_x-offset:right_hand_s1_x+offset, \n",
    "                                     right_hand_s1_y-offset:right_hand_s1_y+offset, \n",
    "                                     :]\n",
    "    with open('pickles/' + short_filename + '_skeleton.pkl', 'wb') as pickle_file:\n",
    "        pickle.dump(skeleton, pickle_file)\n",
    "        \n",
    "    with open('pickles/' + short_filename + '_rgb.pkl', 'wb') as pickle_file:\n",
    "        pickle.dump(hand_crops, pickle_file)\n",
    "        \n",
    "end = time.time()\n",
    "print(\"It took : \" + str(end - start) + \" seconds\")\n",
    "'''\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## h5py approach\n",
    "\n",
    "Takes same time and same space without compression. \n",
    "\n",
    "Takes longer (~15s per 60 samples) when compressing with GZIP, compression_opts=9 but saves ~115MB per 60 samples.\n",
    "\n",
    "Takes longer (4s per 60 samples) when compression with Lzf, but saves 90MB\n",
    "\n",
    "Notes :\n",
    "- S016C003P008R002A059 issue first skeleton too far left could not broadcast input array from shape (114,50,13,3) into shape (114,50,50,3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (0,50,3) into shape (50,50,3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-200f6b56764c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# Create empty np arrays containing crops of hands from videos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# shape (n_frames, number of hands (2 subjects), x, y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mhand_crops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_hands\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskeleton_rgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideodata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrop_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# shape (n_frames, 4, crop_size, crop_size, 3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshort_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ntu-rgb-d/notebooks/utils.py\u001b[0m in \u001b[0;36mextract_hands\u001b[0;34m(skeleton_rgb, videodata, crop_size)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mhand_crops_s\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mleft_hand_x\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mleft_hand_x\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_hand_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mleft_hand_y\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mhand_crops_s\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mright_hand_x\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mright_hand_x\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_hand_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mright_hand_y\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mhand_crops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhand_crops_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (0,50,3) into shape (50,50,3)"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "i = 0\n",
    "with h5py.File(ntu_path + 'datasets.h5', 'w') as hdf:\n",
    "    # Loop through skeleton files\n",
    "    for filename in os.listdir(ntu_path + skeleton_folder):\n",
    "        if i % 100 == 0:\n",
    "            print(i)\n",
    "        i += 1\n",
    "        # Retrieve skeleton data\n",
    "        skeleton = read_xyz(ntu_path + skeleton_folder + filename) # shape (2, max_frame, num_joint=25, n_subjects)\n",
    "        skeleton_rgb = read_color_xy(ntu_path + skeleton_folder + filename) # shape (2, max_frame, num_joint=25, n_subjects)\n",
    "\n",
    "        # Assert skeletons are correct\n",
    "        assert skeleton.shape[1:] == skeleton_rgb.shape[1:]\n",
    "\n",
    "        # Sequence code without extension\n",
    "        short_filename = os.path.splitext(filename)[0]\n",
    "        f = open(ntu_path + \"log.txt\",\"a+\")\n",
    "        f.write(short_filename)\n",
    "        f.write(\"\\r\\n\")\n",
    "        f.close()\n",
    "        # print(short_filename)\n",
    "\n",
    "        # Read cooresponding video\n",
    "        videodata = skvideo.io.vread(ntu_path + rgb_folder + short_filename +'_rgb.avi') # shape (n_frames, 1080, 1920, 3)\n",
    "\n",
    "        # Check that video data has same number of frames as skeleton\n",
    "        assert skeleton.shape[1] == videodata.shape[0]\n",
    "\n",
    "        n_frames = videodata.shape[0]\n",
    "\n",
    "        # Create empty np arrays containing crops of hands from videos\n",
    "        # shape (n_frames, number of hands (2 subjects), x, y)\n",
    "        hand_crops = extract_hands(skeleton_rgb, videodata, crop_size) # shape (n_frames, 4, crop_size, crop_size, 3)\n",
    "            \n",
    "        sample = hdf.create_group(short_filename)\n",
    "        sample.create_dataset(\"skeleton\", data = skeleton, compression=\"gzip\", compression_opts=9)\n",
    "        sample.create_dataset(\"rgb\", data = hand_crops, compression=\"gzip\", compression_opts=9)\n",
    "        \n",
    "end = time.time()\n",
    "print(\"It took : \" + str(end - start) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
